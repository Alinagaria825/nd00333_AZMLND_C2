# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**In 1-2 sentences, explain the problem statement**
This dataset contains data about different informations (Age, Job, Marital, Loan, Contact, etc.) from different individuals. The objective is to predict whether client will subscribe into a term deposit. Automl and Heperdrive were both used with clasfficiation task

**In 1-2 sentences, explain the solution**
First is to create a tabular dataset using TabularDatasetFcoty, then both AutoML and Hyperdrive is used to generate the best model. Both has accuracy as the primary metric. Hyperparameters were tuned and in AutoML the task is classification. The best performing model is generated from AutoML with algorith name of "VotingEnsemble", it has the highest primary metric which was set to be accuracy.

## Scikit-learn Pipeline
**Explain the pipeline architecture, including data, hyperparameter tuning, and classification algorithm.**
The pipeline includes a dataset created from the provided URL using TabularDatasetFactory, then the data was split into train and test set. An SKLearn estimator was defined for use with train.py and the cleaned dataset was fed in to the Logistic Regressio model. Hyperparameter tuning includes RandomPArameterSampling as the parameter sampler with "--C" and "--max_iter" as parameters, SKLearn as estimator, and a BanditPolicy as an early stopping policy.

**What are the benefits of the parameter sampler you chose?**
Random Sampling supports discrete and continuous hyperparameters. It supports early termination of low-performance runs. This is the best parameter sampler since an early stopping policy is used. RandomParameterSampling parameters were set to maximize the primary metric goal.

**What are the benefits of the early stopping policy you chose?**
BanditPolicy class defines an early termination policy based on slack criteria, I used a slack factor of 0.1 an evaluation interval of 2. This means that evaluation will occur every 2 iteration and it will terminate job that falls off below the top 10% with respect to the best performing run.

## AutoML
**In 1-2 sentences, describe the model and hyperparameters generated by AutoML.**
In AutoML, dataset was also created rom the provided URL using TabularDatasetFactory. Hyperparameters include: task as "classification", primary metric of "accuracy" and cross validations of until 5. The models's algorithm names were:VotingEnsemble, LightGBM, XGBoostClassifier, RandomForest, SGD, and ExtremeRandomTrees. The one with best metric was VotingEnsemble with **91.656%** accuracy

## Pipeline comparison
AutoML runs different algorithms for the available dataset while hyperdrive depends on the Scikit-Learn model that was chosen. When comparing the best model in terms with the primary metric there is a little difference, Hyperdrive's best model has an accuracy of **91.10%** while AutoML's best model has an accuracy of **91.66** with a difference of 0.56%. 

In addition, cleaned dataset can be directly fed in AutoML's confguration while in Hyperdrive, the cleaned dataset will first go through Scikit-learn then pass it in hyperdrive configuration. 

## Future work
**What are some areas of improvement for future experiments? Why might these improvements help the model?**
I think in Scikit-learn, hyperparameters can be defined differently by trying to use other parameter sampling and early stopping policy. This might make difference with the best  model since we will be using different type of hyperparameter tuning. Also, it will help students to figure out how selecting parameters affect the accuracy or other metrics.
